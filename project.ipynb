{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"N31dcV0oDy_V"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/Colab Notebooks/final_project'"],"id":"N31dcV0oDy_V"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bd394f20"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n","\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers.legacy import Adam\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.applications import MobileNetV2, ResNet50\n","from tensorflow.keras.models import Model\n","\n","from PIL import Image"],"id":"bd394f20"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d81c23cb"},"outputs":[],"source":["# Load dataset images into the DataFrame.\n","\n","dataset = []\n","\n","for emotion in os.listdir('dataset'):\n","    print(emotion)\n","    if os.path.isdir(f'dataset/{emotion}'):\n","        for image_filename in os.listdir(f'dataset/{emotion}'):\n","            image = Image.open(f'dataset/{emotion}/{image_filename}')\n","            pixels = np.array(image).astype(float)\n","            dataset.append((pixels, emotion))\n","\n","df = pd.DataFrame(dataset, columns=['pixels', 'emotion'])"],"id":"d81c23cb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"68055e3b"},"outputs":[],"source":["df.head()"],"id":"68055e3b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"23be07c0"},"outputs":[],"source":["df['emotion'].unique()"],"id":"23be07c0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2079735d"},"outputs":[],"source":["# Normalization of the data\n","\n","X = np.array([MinMaxScaler().fit_transform(e) for e in df['pixels']])\n","Y = LabelBinarizer().fit_transform(df['emotion'])"],"id":"2079735d"},{"cell_type":"code","source":["X[0].shape"],"metadata":{"id":"CVGQyC_8Besj"},"id":"CVGQyC_8Besj","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f377a2a"},"outputs":[],"source":["# Split dataset into train and test data.\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"],"id":"8f377a2a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e998492e"},"outputs":[],"source":["print(X_train.shape)\n","print(Y_train.shape)\n","print(X_test.shape)\n","print(Y_test.shape)"],"id":"e998492e"},{"cell_type":"code","source":["# Attributes for input (images width, high and channel) and output (number of labels).\n","\n","_, IMAGE_WIDTH, IMAGE_HEIGHT = X.shape\n","_, NUM_LABELS = Y.shape\n","NUM_CHANNELS = 1"],"metadata":{"id":"OHioS07l_K5b"},"id":"OHioS07l_K5b","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3839473d"},"outputs":[],"source":["# Create a Convolutional model.\n","\n","modelConv = Sequential([\n","    Conv2D(32, (3,3), padding=\"Same\", activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS)),\n","    BatchNormalization(),\n","    Conv2D(32, (5,5), padding=\"Same\", activation='relu'),\n","    MaxPooling2D((2,2)),\n","    Dropout(0.5),\n","    Conv2D(64, (3,3), padding=\"Same\", activation='relu'),\n","    BatchNormalization(),\n","    Conv2D(64, (5,5), padding=\"Same\", activation='relu'),\n","    MaxPooling2D((2,2)),\n","    Dropout(0.5),\n","    Conv2D(128, (3,3), padding=\"Same\", activation='relu'),\n","    BatchNormalization(),\n","    Conv2D(128, (5,5), padding=\"Same\", activation='relu'),\n","    MaxPooling2D((2,2)),\n","    Dropout(0.5),\n","    Flatten(),\n","    Dense(256, activation='relu'),\n","    Dense(NUM_LABELS, activation='softmax'),\n","])\n","\n","modelConv.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"],"id":"3839473d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"82b25a21"},"outputs":[],"source":["# Train the convolutional model.\n","\n","modelConv.fit(X_train, Y_train, batch_size=32, epochs=25, validation_data=(X_test, Y_test), callbacks=[TensorBoard(log_dir=\"logs/modelConv\")])"],"id":"82b25a21"},{"cell_type":"code","source":["score, acc = modelConv.evaluate(X_test, Y_test, batch_size=100)\n","print('Test score:', score)\n","print(\"Test accuracy:\", acc)"],"metadata":{"id":"FG0BYiZVdSpi"},"id":"FG0BYiZVdSpi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset images into the DataFrame and convert into RGB (required by MobileNetV2)\n","\n","dataset = []\n","\n","for emotion in os.listdir('dataset'):\n","    if os.path.isdir(f'dataset/{emotion}'):\n","        for image_filename in os.listdir(f'dataset/{emotion}'):\n","          image = Image.open(f'dataset/{emotion}/{image_filename}').convert(\"RGB\")\n","          pixels = np.array(image).astype(float)\n","          dataset.append((pixels, emotion))\n","\n","df = pd.DataFrame(dataset, columns=['pixels', 'emotion'])"],"metadata":{"id":"Z40fGHPS4UtN"},"id":"Z40fGHPS4UtN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Normalizacion of the data.\n","\n","X = np.array((df['pixels']/255).values.tolist())\n","Y = LabelBinarizer().fit_transform(df['emotion'])"],"metadata":{"id":"xp8NChn1_cL8"},"id":"xp8NChn1_cL8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split dataset into train and test data.\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"],"metadata":{"id":"oxcjZco2_et5"},"id":"oxcjZco2_et5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Attributes for input (images width, high and channel) and output (number of labels).\n","\n","_, IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS = X.shape\n","_, NUM_LABELS = Y.shape"],"metadata":{"id":"Bhsvl25o_8vA"},"id":"Bhsvl25o_8vA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model = MobileNetV2(input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS),\n","                         include_top=False,\n","                         weights='imagenet')\n","\n","#base_model.trainable = False\n","\n","modelMobNet = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dropout(0.3),\n","    Dense(128, activation='relu'),\n","    Dense(NUM_LABELS, activation='softmax')\n","])\n","\n","modelMobNet.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"],"metadata":{"id":"LEpIQOGRAAIb"},"id":"LEpIQOGRAAIb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import TensorBoard\n","tensorboarddata = TensorBoard(log_dir='logs/train_borad')"],"metadata":{"id":"NrxGWvKiQUcC"},"id":"NrxGWvKiQUcC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir logs"],"metadata":{"id":"gdefW_91ROeR"},"id":"gdefW_91ROeR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelMobNet.fit(X_train, Y_train, \n","                batch_size=32, \n","                epochs=25,  \n","                validation_data=(X_test, Y_test), \n","                callbacks=[TensorBoard(log_dir=\"logs/modelMobNet\")])\n"],"metadata":{"id":"GIapEIlR4U33"},"id":"GIapEIlR4U33","execution_count":null,"outputs":[]},{"cell_type":"code","source":["score, acc = modelMobNet.evaluate(X_test, Y_test, batch_size=100)\n","print('Test score:', score)\n","print(\"Test accuracy:\", acc)"],"metadata":{"id":"UbzFWdlBdBf1"},"id":"UbzFWdlBdBf1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"erCEsQAhQ4HF"},"id":"erCEsQAhQ4HF"},{"cell_type":"code","source":["base_model = ResNet50(input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS),\n","                      include_top=False, \n","                      weights='imagenet')\n","\n","#base_model.trainable = False\n","\n","modelResNet = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dropout(0.3),\n","    Dense(128, activation='relu'),\n","    Dense(NUM_LABELS, activation='softmax')\n","])\n","\n","modelResNet.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n"],"metadata":{"id":"pP_tU4A6QoYG"},"id":"pP_tU4A6QoYG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelResNet.fit(X_train, Y_train,\n","                batch_size=32, \n","                epochs=25,\n","                validation_data=(X_test, Y_test),\n","                callbacks=[TensorBoard(log_dir=\"logs/modelResNet\")])"],"metadata":{"id":"2-15tGpjb0Zt"},"id":"2-15tGpjb0Zt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["score, acc = modelResNet.evaluate(X_test, Y_test, batch_size=100)\n","print('Test score:', score)\n","print(\"Test accuracy:\", acc)"],"metadata":{"id":"ELsngb58bQfd"},"id":"ELsngb58bQfd","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","private_outputs":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}