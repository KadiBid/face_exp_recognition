{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"N31dcV0oDy_V"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/Colab Notebooks/final_project'"],"id":"N31dcV0oDy_V"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bd394f20"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n","\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers.legacy import Adam\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.applications import MobileNetV2, ResNet50\n","from keras.applications.vgg16 import VGG16\n","\n","import matplotlib.pyplot as plt\n","from mlxtend.plotting import plot_confusion_matrix\n","\n","from PIL import Image\n"],"id":"bd394f20"},{"cell_type":"code","source":["# List of emotions on the dataset we want to include (using comments -#- to exlcude).\n","\n","EMOTIONS=[\n","    'fear',\n","    'disgust',\n","    'neutral',\n","    'happy',\n","    'angry'\n","]"],"metadata":{"id":"0I7eN1Q99sFJ"},"id":"0I7eN1Q99sFJ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d81c23cb"},"outputs":[],"source":["# Load dataset images into the DataFrame.\n","\n","dataset = []\n","\n","for emotion in EMOTIONS:\n","    print(emotion)\n","    if os.path.isdir(f'dataset/{emotion}'):\n","        for image_filename in os.listdir(f'dataset/{emotion}'):\n","            image = Image.open(f'dataset/{emotion}/{image_filename}')\n","            pixels = np.array(image).astype(float)\n","            dataset.append((pixels, emotion))\n","\n","df = pd.DataFrame(dataset, columns=['pixels', 'emotion'])"],"id":"d81c23cb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"68055e3b"},"outputs":[],"source":["df.head()"],"id":"68055e3b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"23be07c0"},"outputs":[],"source":["df['emotion'].unique()"],"id":"23be07c0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2079735d"},"outputs":[],"source":["# Normalization of the data\n","\n","X = np.array([MinMaxScaler().fit_transform(e) for e in df['pixels']])\n","Y = LabelBinarizer().fit_transform(df['emotion'])"],"id":"2079735d"},{"cell_type":"code","source":["X[0].shape"],"metadata":{"id":"CVGQyC_8Besj"},"id":"CVGQyC_8Besj","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f377a2a"},"outputs":[],"source":["# Split dataset into train and test data.\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"],"id":"8f377a2a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e998492e"},"outputs":[],"source":["print(X_train.shape)\n","print(Y_train.shape)\n","print(X_test.shape)\n","print(Y_test.shape)"],"id":"e998492e"},{"cell_type":"code","source":["# Attributes for input (images width, high and channel) and output (number of labels).\n","\n","_, IMAGE_WIDTH, IMAGE_HEIGHT = X.shape\n","_, NUM_LABELS = Y.shape\n","NUM_CHANNELS = 1"],"metadata":{"id":"OHioS07l_K5b"},"id":"OHioS07l_K5b","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3839473d"},"outputs":[],"source":["# Create a Convolutional model.\n","\n","modelConv = Sequential([\n","    Conv2D(32, (3,3), padding=\"Same\", activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS)),\n","    BatchNormalization(),\n","    Conv2D(32, (5,5), padding=\"Same\", activation='relu'),\n","    MaxPooling2D((2,2)),\n","    Dropout(0.5),\n","    Conv2D(64, (3,3), padding=\"Same\", activation='relu'),\n","    BatchNormalization(),\n","    Conv2D(64, (5,5), padding=\"Same\", activation='relu'),\n","    MaxPooling2D((2,2)),\n","    Dropout(0.5),\n","    Conv2D(128, (3,3), padding=\"Same\", activation='relu'),\n","    BatchNormalization(),\n","    Conv2D(128, (5,5), padding=\"Same\", activation='relu'),\n","    MaxPooling2D((2,2)),\n","    Dropout(0.5),\n","    Flatten(),\n","    Dense(256, activation='relu'),\n","    Dense(NUM_LABELS, activation='softmax'),\n","])\n","\n","modelConv.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"],"id":"3839473d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"82b25a21"},"outputs":[],"source":["# Train the convolutional model.\n","\n","historyConv = modelConv.fit(X_train, Y_train, \n","                            batch_size=32, epochs=25, \n","                            validation_data=(X_test, Y_test), \n","                            callbacks=[TensorBoard(log_dir=\"logs/modelConv\")])"],"id":"82b25a21"},{"cell_type":"code","source":["score, acc = modelConv.evaluate(X_test, Y_test, batch_size=100)\n","print('Test score:', score)\n","print(\"Test accuracy:\", acc)"],"metadata":{"id":"FG0BYiZVdSpi"},"id":"FG0BYiZVdSpi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset images into the DataFrame and convert into RGB (required by MobileNetV2)\n","\n","dataset = []\n","\n","for emotion in EMOTIONS:\n","    print(emotion)\n","    if os.path.isdir(f'dataset/{emotion}'):\n","        for image_filename in os.listdir(f'dataset/{emotion}'):\n","          image = Image.open(f'dataset/{emotion}/{image_filename}').convert(\"RGB\")\n","          pixels = np.array(image).astype(float)\n","          dataset.append((pixels, emotion))\n","\n","df = pd.DataFrame(dataset, columns=['pixels', 'emotion'])"],"metadata":{"id":"Z40fGHPS4UtN"},"id":"Z40fGHPS4UtN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Normalizacion of the data.\n","\n","X = np.array((df['pixels']/255).values.tolist())\n","Y = LabelBinarizer().fit_transform(df['emotion'])"],"metadata":{"id":"xp8NChn1_cL8"},"id":"xp8NChn1_cL8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split dataset into train and test data.\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"],"metadata":{"id":"oxcjZco2_et5"},"id":"oxcjZco2_et5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Attributes for input (images width, high and channel) and output (number of labels).\n","\n","_, IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS = X.shape\n","_, NUM_LABELS = Y.shape"],"metadata":{"id":"Bhsvl25o_8vA"},"id":"Bhsvl25o_8vA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model = MobileNetV2(input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS),\n","                         include_top=False,\n","                         weights='imagenet')\n","\n","\n","modelMobNet = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dropout(0.3),\n","    Dense(128, activation='relu'),\n","    Dense(NUM_LABELS, activation='softmax')\n","])\n","\n","modelMobNet.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])"],"metadata":{"id":"LEpIQOGRAAIb"},"id":"LEpIQOGRAAIb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the MobileNetV2 based model.\n","\n","historyMobNet = modelMobNet.fit(X_train, Y_train,\n","                                batch_size=32, epochs=25, \n","                                validation_data=(X_test, Y_test),\n","                                callbacks=[TensorBoard(log_dir=\"logs/modelMobNet\")])\n"],"metadata":{"id":"GIapEIlR4U33"},"id":"GIapEIlR4U33","execution_count":null,"outputs":[]},{"cell_type":"code","source":["score, acc = modelMobNet.evaluate(X_test, Y_test, batch_size=100)\n","print('Test score:', score)\n","print(\"Test accuracy:\", acc)"],"metadata":{"id":"UbzFWdlBdBf1"},"id":"UbzFWdlBdBf1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"erCEsQAhQ4HF"},"id":"erCEsQAhQ4HF"},{"cell_type":"code","source":["base_model = ResNet50(input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS),\n","                      include_top=False, \n","                      weights='imagenet')\n","\n","base_model.trainable = False\n","\n","modelResNet = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dropout(0.3),\n","    Dense(128, activation='relu'),\n","    Dense(NUM_LABELS, activation='softmax')\n","])\n","\n","modelResNet.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n"],"metadata":{"id":"pP_tU4A6QoYG"},"id":"pP_tU4A6QoYG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the ResNet50 based model.\n","\n","historyResNet = modelResNet.fit(X_train, Y_train,\n","                                batch_size=32, epochs=25, \n","                                validation_data=(X_test, Y_test),\n","                                callbacks=[TensorBoard(log_dir=\"logs/modelResNet\")])"],"metadata":{"id":"2-15tGpjb0Zt"},"id":"2-15tGpjb0Zt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["score, acc = modelResNet.evaluate(X_test, Y_test, batch_size=100)\n","print('Test score:', score)\n","print(\"Test accuracy:\", acc)"],"metadata":{"id":"ELsngb58bQfd"},"id":"ELsngb58bQfd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a VGG model.\n","base_model = VGG16(input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS),\n","                      include_top=False, \n","                      weights='imagenet')\n","\n","base_model.trainable = False\n","\n","modelVGG = Sequential([\n","    base_model,\n","    Flatten(),\n","    Dense(256, activation=\"relu\"),\n","    Dropout(0.5),\n","    Dense(NUM_LABELS, activation=\"softmax\")\n","])\n","\n","# Compilar el modelo\n","modelVGG.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"metadata":{"id":"ceStGrlNfVhR"},"id":"ceStGrlNfVhR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the MobileVGG based model.\n","\n","historyVGG = modelVGG.fit(X_train, Y_train,\n","                                batch_size=32, epochs=25, \n","                                validation_data=(X_test, Y_test),\n","                                callbacks=[TensorBoard(log_dir=\"logs/modelVGG\")])"],"metadata":{"id":"LPOrAxMKfWdP"},"id":"LPOrAxMKfWdP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["score, acc = modelVGG.evaluate(X_test, Y_test, batch_size=100)\n","print('Test score:', score)\n","print(\"Test accuracy:\", acc)"],"metadata":{"id":"5rl4vTe7fXAE"},"id":"5rl4vTe7fXAE","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sk3gbusU9QDM"},"id":"sk3gbusU9QDM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show lost and accuracy for train and test for the convolutional network.\n","\n","fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n","axs[0].plot(historyConv.history['loss'])\n","axs[0].plot(historyConv.history['val_loss'])\n","axs[1].plot(historyConv.history['accuracy'])\n","axs[1].plot(historyConv.history['val_accuracy'])"],"metadata":{"id":"d_F4Tej89Qla"},"id":"d_F4Tej89Qla","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show lost and accuracy for train and test for the MobileNetv2 based network.\n","\n","fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n","axs[0].plot(historyMobNet.history['loss'])\n","axs[0].plot(historyMobNet.history['val_loss'])\n","axs[1].plot(historyMobNet.history['accuracy'])\n","axs[1].plot(historyMobNet.history['val_accuracy'])"],"metadata":{"id":"-vFnWvDN9QmS"},"id":"-vFnWvDN9QmS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show lost and accuracy for train and test for the ResNet50 based network.\n","\n","fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n","axs[0].plot(historyResNet.history['loss'])\n","axs[0].plot(historyResNet.history['val_loss'])\n","axs[1].plot(historyResNet.history['accuracy'])\n","axs[1].plot(historyResNet.history['val_accuracy'])"],"metadata":{"id":"uuHD9yFp9Qxg"},"id":"uuHD9yFp9Qxg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show lost and accuracy for train and test for the ResNet50 based network.\n","\n","fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n","axs[0].plot(historyVGG.history['loss'])\n","axs[0].plot(historyVGG.history['val_loss'])\n","axs[1].plot(historyVGG.history['accuracy'])\n","axs[1].plot(historyVGG.history['val_accuracy'])"],"metadata":{"id":"ci4vLDmV9Q0X"},"id":"ci4vLDmV9Q0X","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-mXc3i0D9qu5"},"id":"-mXc3i0D9qu5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save trained models\n","\n","modelConv.save('models/modelConv_cleaned.h5')\n","modelMobNet.save('models/modelMobNet_cleaned.h5')\n","modelResNet.save('models/modelResNet_cleaned.h5')\n","modelVGG.save('models/modelVGG_cleaned.h5')\n"],"metadata":{"id":"LG-mKr1x9Q5V"},"id":"LG-mKr1x9Q5V","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","private_outputs":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}